{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the unstructured data into a RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring non-Spark config property: sudo\n",
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SN19710:0 2000-01-01T06:00:00.000Z air_temperature:-6.3degC height_above_ground:2m PT0H PT6H 0 C 2 2\n",
      "SN19710:0 2000-01-01T06:00:00.000Z air_temperature:-6.3degC height_above_ground:2m PT0H PT6H 0 C 2 2\n",
      "SN19710:0 2000-01-01T06:00:00.000Z air_temperature:-6.3degC height_above_ground:2m PT0H PT6H 0 C 2 2\n",
      "SN19710:0 2000-01-01T06:00:00.000Z air_temperature:-6.3degC height_above_ground:2m PT0H PT6H 0 C 2 2\n",
      "SN19710:0 2000-01-01T06:00:00.000Z air_temperature:-6.3degC height_above_ground:2m PT0H PT6H 0 C 2 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "# Best spark configuration for the given data that we tested\n",
    "spark = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"data_cleaning\")\n",
    "    .config(\"spark.master\", \"yarn\")\n",
    "    .config(\"spark.driver.memory\", \"2g\")\n",
    "    .config(\"spark.yarn.am.memory\", \"2g\")\n",
    "    .config(\"spark.executor.memory\", \"4g\")\n",
    "    .config(\"spark.executor.cores\", \"4\")\n",
    "    .config(\"spark.executor.instances\", \"3\")\n",
    "    .config(\"spark.task.cpus\", \"1\")\n",
    "    .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "    .getOrCreate()\n",
    ")\n",
    "\n",
    "loaded_rdd = spark.sparkContext.textFile(\"hdfs:///project/raw_temperature_data\")\n",
    "\n",
    "# Printing the first 5 entries\n",
    "for i in loaded_rdd.take(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the raw data line by line and convert it into a structured format containing date as the key and temperature and since cosine tranformation as values. Since the data is hourly we convert the datetime to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 0.0025539398193359375\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2000-01-01', (-6.3, 0.02, 1.0))\n",
      "('2000-01-01', (-6.3, 0.02, 1.0))\n",
      "('2000-01-01', (-6.3, 0.02, 1.0))\n",
      "('2000-01-01', (-6.3, 0.02, 1.0))\n",
      "('2000-01-01', (-6.3, 0.02, 1.0))\n",
      "('2000-01-01', (-6.3, 0.02, 1.0))\n",
      "('2000-01-01', (-6.3, 0.02, 1.0))\n",
      "('2000-01-01', (-6.3, 0.02, 1.0))\n",
      "('2000-01-01', (-6.3, 0.02, 1.0))\n",
      "('2000-01-01', (-6.3, 0.02, 1.0))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "import time\n",
    "\n",
    "# Defining a function to process each line of the log file and extract date as a key and temperature and day of the year as values\n",
    "def process_line(line):\n",
    "    fields = line.split(\" \")\n",
    "    \n",
    "    source_id, ref_time, temperature, height, time_offset, time_res, time_SeriesId,  performanceCategory,  exposureCategory,  qualityCode = [f.strip() for f in fields]\n",
    "    \n",
    "    # date as format: 2024-10-01T00:00:00.000Z\n",
    "    date = ref_time.split(\"T\")[0]\n",
    "    temp_value = float(temperature.split(\":\")[1].rstrip(\"degC\"))\n",
    "\n",
    "    # Converting to datetime object\n",
    "    date_obj = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "    # Finding the day of the year\n",
    "    day_of_year = date_obj.timetuple().tm_yday\n",
    "\n",
    "    # Applying sine/cosine transformation to day of the year to capture seasonality\n",
    "    day_of_year_sin = round(math.sin(2 * math.pi * day_of_year / 365), 2)\n",
    "    day_of_year_cos = round(math.cos(2 * math.pi * day_of_year / 365), 2)\n",
    "    \n",
    "    return (date, (temp_value, day_of_year_sin, day_of_year_cos))\n",
    "\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "processed_rdd = loaded_rdd.map(process_line)\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Execution time: {execution_time}\")\n",
    "\n",
    "# Printing the first 10 entries\n",
    "for i in processed_rdd.take(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the data is in hourly format, and we above converted it to daily format, we need to aggregate the data to get the average temperature for each day.\n",
    "\n",
    "Tested with two different algorithms using groupbyKey and reduceByKey. The reduceByKey is faster and more efficient so we will use that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:======================================================>  (22 + 1) / 23]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2013-01-17', (-8, 0.29, 0.96))\n",
      "('2013-01-25', (-13, 0.42, 0.91))\n",
      "('2013-02-10', (-7, 0.65, 0.76))\n",
      "('2014-05-09', (6, 0.8, -0.61))\n",
      "('2014-06-14', (15, 0.3, -0.95))\n",
      "('2014-06-16', (15, 0.26, -0.96))\n",
      "('2014-08-04', (19, -0.55, -0.84))\n",
      "('2014-08-27', (16, -0.83, -0.56))\n",
      "('2014-09-13', (14, -0.95, -0.3))\n",
      "('2014-09-16', (13, -0.97, -0.25))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import statistics\n",
    "\n",
    "# Algo 1\n",
    "# Grouping the temperatures by date and calculating the daily average temperature\n",
    "# daily_avg_rdd = processed_rdd.groupByKey().mapValues(lambda temps: list(temps)) \\\n",
    "#     .mapValues(lambda temps: (round(statistics.mean([t[0] for t in temps])), temps[0][1], temps[0][2]))\n",
    "\n",
    "# Algo 2\n",
    "daily_avg_rdd = processed_rdd \\\n",
    "    .map(lambda t: (t[0], (t[1][0], t[1][1], t[1][2], 1))) \\\n",
    "    .reduceByKey(lambda a, b: (a[0] + b[0], a[1], a[2], a[3] + b[3])) \\\n",
    "    .mapValues(lambda v: (round(v[0] / v[3]), v[1], v[2]))\n",
    "\n",
    "# Printing the first 10 entries\n",
    "for i in daily_avg_rdd.take(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert cleaned data to a data frame and save it as a parquet file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Execution time: 6.480842113494873 seconds\n",
      "+----------+-----------+--------+--------+\n",
      "|      date|temperature|feature1|feature2|\n",
      "+----------+-----------+--------+--------+\n",
      "|2000-01-01|         -6|    0.02|     1.0|\n",
      "|2000-01-02|         -1|    0.03|     1.0|\n",
      "|2000-01-03|          2|    0.05|     1.0|\n",
      "|2000-01-04|          2|    0.07|     1.0|\n",
      "|2000-01-05|         -1|    0.09|     1.0|\n",
      "|2000-01-06|          5|     0.1|    0.99|\n",
      "|2000-01-07|          4|    0.12|    0.99|\n",
      "|2000-01-08|          6|    0.14|    0.99|\n",
      "|2000-01-09|          1|    0.15|    0.99|\n",
      "|2000-01-10|          0|    0.17|    0.99|\n",
      "|2000-01-11|          3|    0.19|    0.98|\n",
      "|2000-01-12|          2|    0.21|    0.98|\n",
      "|2000-01-13|          1|    0.22|    0.98|\n",
      "|2000-01-14|         -2|    0.24|    0.97|\n",
      "|2000-01-15|         -3|    0.26|    0.97|\n",
      "|2000-01-16|          0|    0.27|    0.96|\n",
      "|2000-01-17|          8|    0.29|    0.96|\n",
      "|2000-01-18|          3|     0.3|    0.95|\n",
      "|2000-01-19|          1|    0.32|    0.95|\n",
      "|2000-01-20|          4|    0.34|    0.94|\n",
      "+----------+-----------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "import pyspark.sql.functions as F\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "formatted_rdd = daily_avg_rdd.map(lambda x: (x[0], x[1][0], x[1][1], x[1][2]))\n",
    "\n",
    "# Define the schema for your data\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"temperature\", IntegerType(), True),\n",
    "    StructField(\"feature1\", FloatType(), True),\n",
    "    StructField(\"feature2\", FloatType(), True)\n",
    "])\n",
    "\n",
    "## Convert the RDD to a DataFrame\n",
    "df = spark.createDataFrame(formatted_rdd, schema)\n",
    "\n",
    "# Order the DataFrame by date so the data is in chronological order\n",
    "df = df.sort(F.col(\"date\"))\n",
    "\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(\"hdfs:///project/cleaned_data_parquet\")\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Execution time: {execution_time} seconds\")\n",
    "\n",
    "df.show()\n",
    "spark.stop()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Find the size of the cleaned data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45.1 K  135.2 K  /project/cleaned_data_parquet\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Size of the cleaned data\n",
    "import os\n",
    "import sys\n",
    "\n",
    "# Get the size of the cleaned data\n",
    "size = os.popen(\"hdfs dfs -du -s -h /project/cleaned_data_parquet\").read()\n",
    "print(size)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
