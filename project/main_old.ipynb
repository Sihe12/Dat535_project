{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project\n",
    "## Problem Statement\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Data Ingestion\n",
    "\n",
    "### Description\n",
    "\n",
    "In this phase, we focus on the ingestion of raw weather data into the data pipeline. The raw data consists of historical hourly temperature readings from multiple weather stations, which will be used later for building a machine learning model to predict future temperatures. \n",
    "\n",
    "We use Spark's distributed computing capabilities to handle the ingestion of large datasets, ensuring scalability and efficiency, especially as the dataset grows over time. \n",
    "\n",
    "To make the dataset more challenging and to simulate real-world data processing, we artificially \"unstructured\" the JSON data into a raw, log-like format. This requires additional preprocessing to transform it back into a usable format. \n",
    "\n",
    "#### Steps in the Ingestion Process\n",
    "\n",
    "1. **API Data Source**:\n",
    "   - We retrieve temperature data from the Frost API, which provides detailed weather observations. The raw dataset includes hourly temperature readings, metadata about the measurement conditions, and timestamps.\n",
    "   - The raw data is ingested into our Spark cluster, ensuring it is distributed across nodes for efficient processing.\n",
    "\n",
    "2. **Simulating Raw Data**:\n",
    "   - Although the API provides semi-structured JSON data, we convert it into a less structured log format. This simulates the ingestion of raw data that requires significant preprocessing. An example of the raw data format looks as follows:\n",
    "     ```\n",
    "     SN44640:0 | 2024-01-01T00:00:00Z | air_temperature:3.5degC | height:2m\n",
    "     SN44640:0 | 2024/01/01 01:00:00 | 3.5 deg C, height: 2 m\n",
    "     ```\n",
    "   - The structure has been intentionally removed to introduce complexity, making it harder to directly extract relevant fields like `temperature` or `timestamp`.\n",
    "\n",
    "3. **Cloud Environment Setup**:\n",
    "   - A cloud infrastructure is established on [Cloud Provider], with a Spark cluster set up to handle distributed data processing. We use the following services:\n",
    "     - **Storage**: For storing raw, intermediate, and processed datasets.\n",
    "     - **Compute**: A Spark cluster for parallel data processing.\n",
    "   - The cloud setup ensures that the system can scale horizontally as more data is ingested.\n",
    "\n",
    "4. **Data Ingestion into Spark**:\n",
    "   - The raw, unstructured log data is ingested into the Spark environment using the `spark.read.text()` function, which loads the raw logs into a DataFrame for further processing.\n",
    "   - The ingestion pipeline is capable of handling both real-time streaming data (if the API provides live updates) and batch loading for historical data.\n",
    "\n",
    "#### Challenges Encountered\n",
    "- **Handling Unstructured Data**: Due to the simulated raw format, it was necessary to design robust parsing routines that could handle variations in formatting (e.g., inconsistent delimiters or missing fields).\n",
    "- **Scaling for Large Datasets**: Given the potential size of the dataset, we optimized the ingestion process to handle large volumes of data by leveraging Spark's distributed capabilities.\n",
    "\n",
    "#### Ingestion Outcome\n",
    "- By the end of this step, the raw data is successfully ingested into the Spark cluster, ready for the next phase of **Data Cleaning**. Each record includes essential fields such as temperature, timestamp, and weather station metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the url for the API call\n",
    "SN44640 and SN44630 is the station id for the weather station at Stavanger - Våland and Stavanger - Kiellandsmyra respectively. Achived from https://seklima.met.no/. \n",
    "\n",
    "The data is fetched from the frost.met.no API. The data is fetched from the API using the following parameters:\n",
    "- Sources: SN44640 and SN44630\n",
    "- From date: 2010-01-01\n",
    "- To date: 2024-10-04\n",
    "- Elements: air_temperature, wind_speed, precipitation_amount, and relative_humidity\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stationNr = \"SN44630, SN44640\" usikker på om to egt er nødvendig. Kan evt ta averagen av de to. Hvis vi bare tar 1 kan vi hente data for en lengre periode.\n",
    "stationNr = \"SN44640\"\n",
    "\n",
    "#fromDate = \"2015-01-01\"\n",
    "fromDate = \"2024-10-05\"\n",
    "toDate = \"2024-10-09\"\n",
    "\n",
    "elements = \"air_temperature\"\n",
    "\n",
    "url = \"https://frost.met.no/observations/v0.jsonld?sources=\" + stationNr + \"&referencetime=\" + fromDate + \"/\" + toDate + \"&elements=\"+ elements\n",
    "username = \"09e81c81-f133-474d-a678-7929cb28b4ef:5291a085-b859-4490-9a99-6539c879a165\"\n",
    "password = \"5291a085-b859-4490-9a99-6539c879a165\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"TemperatureDataIngestion\").getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the data from the API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "result = requests.get(url, auth=(username, password))\n",
    "data = result.json()\n",
    "data_entries = data['data']\n",
    "rdd = spark.sparkContext.parallelize(data_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the data is semi-structured, we need to convert it to a raw format and save it in a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_raw_format(entry):\n",
    "    source_id = entry['sourceId']\n",
    "    ref_time = entry['referenceTime']\n",
    "    obs = entry['observations'][0]\n",
    "    temp_value = obs['value']\n",
    "    temp_unit = obs['unit']\n",
    "    height = obs['level']['value']\n",
    "    height_unit = obs['level']['unit']\n",
    "    time_res = obs['timeResolution']\n",
    "    \n",
    "    log_entry = f\"{source_id}  {ref_time}  air_temperature:{temp_value}{temp_unit}  height_above_ground:{height}{height_unit}  {time_res}\"\n",
    "    return log_entry\n",
    "    \n",
    "raw_rdd = rdd.map(to_raw_format)\n",
    "\n",
    "raw_rdd.saveAsTextFile(\"hdfs:///project/raw_temperature_data\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the saved text file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the saved data from the directory\n",
    "loaded_rdd = spark.sparkContext.textFile(\"hdfs:///project/raw_temperature_data\")\n",
    "\n",
    "# Print the first 5 entries to verify it was saved and loaded correctly\n",
    "print(loaded_rdd.take(5))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Data Cleaning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the raw data line by line and convert it into a structured format containing date as the ky and temperature and day of the year as values. Since the data is hourly we convert the datetime to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "# Define a function to process each line of the log file and extract date as a key and temperature and day of the year as values\n",
    "def process_line(line):\n",
    "    fields = line.split('  ')\n",
    "    if len(fields) != 5:\n",
    "        return None  # Invalid format, skip this line\n",
    "\n",
    "    source_id, ref_time, temperature, height, time_res = [f.strip() for f in fields]\n",
    "\n",
    "    # Extract date and temperature\n",
    "    date = ref_time.split('T')[0]  # Assumes ISO format like '2024-10-01T00:00:00.000Z'\n",
    "    temp_value = float(temperature.split(':')[1].rstrip('degC'))\n",
    "\n",
    "    # Convert date string to datetime object\n",
    "    date_obj = datetime.strptime(date, '%Y-%m-%d')\n",
    "\n",
    "    # Calculate day of the year\n",
    "    day_of_year = date_obj.timetuple().tm_yday\n",
    "\n",
    "    # Apply sine/cosine transformation to day of the year to capture seasonality\n",
    "    day_of_year_sin = math.sin(2 * math.pi * day_of_year / 365)\n",
    "    day_of_year_cos = math.cos(2 * math.pi * day_of_year / 365)\n",
    "    \n",
    "    return (date, (temp_value, day_of_year_sin, day_of_year_cos))\n",
    "\n",
    "    \n",
    "\n",
    "    # Process the loaded RDD to extract the required information\n",
    "    \n",
    "\n",
    "processed_rdd = loaded_rdd.map(process_line).filter(lambda x: x is not None)\n",
    "print(processed_rdd.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the data is in hourly format, and we above converted it to daily format, we need to aggregate the data to get the average temperature for each day.\n",
    "\n",
    "### Profile the performance of the MapReduce implementation (e.g., Spark job execution time, memory usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import statistics\n",
    "\n",
    "# Measure the execution time of the MapReduce job\n",
    "start_time = time.time()\n",
    "\n",
    "import statistics\n",
    "\n",
    "# Perform the MapReduce operation with rounding to whole numbers\n",
    "daily_avg_rdd = processed_rdd.groupByKey().mapValues(lambda temps: list(temps)) \\\n",
    "    .mapValues(lambda temps: (round(statistics.mean([t[0] for t in temps])), temps[0][1], temps[0][2]))\n",
    "\n",
    "\n",
    "# # Trigger an action to force execution\n",
    "# daily_avg_rdd.count()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"MapReduce job execution time: {execution_time} seconds\")\n",
    "\n",
    "# To monitor memory usage, you can use the Spark UI. \n",
    "# The Spark UI is usually available at http://<driver-node>:4040\n",
    "# You can access it by navigating to the URL in your web browser.\n",
    "\n",
    "print(daily_avg_rdd.take(5))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there is gaps in the data, fill the missing values with the average of the previous and next value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_max_dates = processed_rdd.map(lambda x: x[1][1]).min(), processed_rdd.map(lambda x: x[1][1]).max()\n",
    "start_day, end_day = min_max_dates\n",
    "complete_days = spark.sparkContext.parallelize(range(start_day, end_day + 1))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
