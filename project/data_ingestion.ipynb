{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 - Data Ingestion\n",
    "\n",
    "### Description\n",
    "\n",
    "In this phase, we focus on the ingestion of raw weather data into the data pipeline. The raw data consists of historical hourly temperature readings from multiple weather stations, which will be used later for building a machine learning model to predict future temperatures. \n",
    "\n",
    "We use Spark's distributed computing capabilities to handle the ingestion of large datasets, ensuring scalability and efficiency, especially as the dataset grows over time. \n",
    "\n",
    "To make the dataset more challenging and to simulate real-world data processing, we artificially \"unstructured\" the JSON data into a raw, log-like format. This requires additional preprocessing to transform it back into a usable format. \n",
    "\n",
    "#### Steps in the Ingestion Process\n",
    "\n",
    "1. **API Data Source**:\n",
    "   - We retrieve temperature data from the Frost API, which provides detailed weather observations. The raw dataset includes hourly temperature readings, metadata about the measurement conditions, and timestamps.\n",
    "   - The raw data is ingested into our Spark cluster, ensuring it is distributed across nodes for efficient processing.\n",
    "\n",
    "2. **Simulating Raw Data**:\n",
    "   - Although the API provides semi-structured JSON data, we convert it into a less structured log format. This simulates the ingestion of raw data that requires significant preprocessing. An example of the raw data format looks as follows:\n",
    "     ```\n",
    "     SN44640:0  2024-01-01T00:00:00Z  air_temperature:3.5degC  height:2m\n",
    "     SN44640:0  2024/01/01 01:00:00  3.5 deg C height: 2 m\n",
    "     ```\n",
    "   - The structure has been intentionally removed to introduce complexity, making it harder to directly extract relevant fields like `temperature` or `timestamp`.\n",
    "\n",
    "3. **Cloud Environment Setup**:\n",
    "   - A cloud infrastructure is established on [Cloud Provider], with a Spark cluster set up to handle distributed data processing. We use the following services:\n",
    "     - **Storage**: For storing raw, intermediate, and processed datasets.\n",
    "     - **Compute**: A Spark cluster for parallel data processing.\n",
    "   - The cloud setup ensures that the system can scale horizontally as more data is ingested.\n",
    "\n",
    "4. **Data Ingestion into Spark**:\n",
    "   - The raw, unstructured log data is ingested into the Spark environment using the `spark.read.text()` function, which loads the raw logs into a DataFrame for further processing.\n",
    "   - The ingestion pipeline is capable of handling both real-time streaming data (if the API provides live updates) and batch loading for historical data.\n",
    "\n",
    "#### Challenges Encountered\n",
    "- **Handling Unstructured Data**: Due to the simulated raw format, it was necessary to design robust parsing routines that could handle variations in formatting (e.g., inconsistent delimiters or missing fields).\n",
    "- **Scaling for Large Datasets**: Given the potential size of the dataset, we optimized the ingestion process to handle large volumes of data by leveraging Spark's distributed capabilities.\n",
    "\n",
    "#### Ingestion Outcome\n",
    "- By the end of this step, the raw data is successfully ingested into the Spark cluster, ready for the next phase of **Data Cleaning**. Each record includes essential fields such as temperature, timestamp, and weather station metadata.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the url for the API call\n",
    "SN19710 is the station id for the weather station at Asker. Choosen because it is a weather station with a long history Achived from https://seklima.met.no/. \n",
    "\n",
    "The data is fetched from the frost.met.no API. The data is fetched from the API using the following parameters:\n",
    "- Sources: SN19710\n",
    "- From date: 2010-01-01\n",
    "- To date: 2024-10-04\n",
    "- Elements: air_temperature, wind_speed, precipitation_amount, and relative_humidity\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fetch the data from the API. Have to split it up since the api only allows for 10000 records at a time."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of data entries:  121414\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "\n",
    "username = \"09e81c81-f133-474d-a678-7929cb28b4ef:5291a085-b859-4490-9a99-6539c879a165\"\n",
    "password = \"5291a085-b859-4490-9a99-6539c879a165\"\n",
    "\n",
    "stationNr = \"SN19710\"\n",
    "\n",
    "elements = \"air_temperature\"\n",
    "\n",
    "data_entries = []\n",
    "\n",
    "#Fetch data from 2000-2010\n",
    "fromDate = \"2000-01-01\"\n",
    "toDate = \"2010-12-31\"\n",
    "url1 = \"https://frost.met.no/observations/v0.jsonld?sources=\" + stationNr + \"&referencetime=\" + fromDate + \"/\" + toDate + \"&elements=\"+ elements\n",
    "\n",
    "result = requests.get(url1, auth=(username, password))\n",
    "data = result.json()\n",
    "data_entries.extend(data[\"data\"])\n",
    "\n",
    "#Fetch data from 2011-2020\n",
    "fromDate = \"2011-01-01\"\n",
    "toDate = \"2020-12-31\"\n",
    "url2 = \"https://frost.met.no/observations/v0.jsonld?sources=\" + stationNr + \"&referencetime=\" + fromDate + \"/\" + toDate + \"&elements=\"+ elements\n",
    "\n",
    "result = requests.get(url2, auth=(username, password))\n",
    "data = result.json()\n",
    "data_entries.extend(data[\"data\"])\n",
    "\n",
    "#Fetch data from 2021-2024\n",
    "fromDate = \"2021-01-01\"\n",
    "toDate = \"2024-10-18\"\n",
    "url3 = \"https://frost.met.no/observations/v0.jsonld?sources=\" + stationNr + \"&referencetime=\" + fromDate + \"/\" + toDate + \"&elements=\"+ elements\n",
    "\n",
    "result = requests.get(url3, auth=(username, password))\n",
    "data = result.json()\n",
    "data_entries.extend(data[\"data\"])\n",
    "\n",
    "print(\"Total number of data entries: \", len(data_entries))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load it into a RDD in Spark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"data_ingestion\").getOrCreate()\n",
    "\n",
    "rdd = spark.sparkContext.parallelize(data_entries)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the data is semi-structured, we need to convert it to a raw format and save it in a txt file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File already exists\n",
      "Deleted /project/raw_temperature_data\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "def to_raw_format(entry):\n",
    "    source_id = entry[\"sourceId\"]\n",
    "    ref_time = entry[\"referenceTime\"]\n",
    "    obs = entry[\"observations\"][0]\n",
    "    temp_value = obs[\"value\"]\n",
    "    temp_unit = obs[\"unit\"]\n",
    "    height = obs[\"level\"][\"value\"]\n",
    "    height_unit = obs[\"level\"][\"unit\"]\n",
    "    time_res = obs[\"timeResolution\"]\n",
    "    \n",
    "    log_entry = f\"{source_id}  {ref_time}  air_temperature:{temp_value}{temp_unit}  height_above_ground:{height}{height_unit}  {time_res}\"\n",
    "    return log_entry\n",
    "    \n",
    "raw_rdd = rdd.map(to_raw_format)\n",
    "\n",
    "try:\n",
    "    #Save it and overwrite if it already exists    \n",
    "    raw_rdd.saveAsTextFile(\"hdfs:///project/raw_temperature_data\")\n",
    "except Exception as e:\n",
    "    print(\"File already exists\")\n",
    "    #Delete the file and save it again\n",
    "    !hdfs dfs -rm -r /project/raw_temperature_data\n",
    "    raw_rdd.saveAsTextFile(\"hdfs:///project/raw_temperature_data\")\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SN19710:0  2000-01-01T06:00:00.000Z  air_temperature:-6.3degC  height_above_ground:2m  PT6H\n",
      "SN19710:0  2000-01-01T12:00:00.000Z  air_temperature:-4degC  height_above_ground:2m  PT6H\n",
      "SN19710:0  2000-01-01T18:00:00.000Z  air_temperature:-7degC  height_above_ground:2m  PT6H\n",
      "SN19710:0  2000-01-02T06:00:00.000Z  air_temperature:-4.4degC  height_above_ground:2m  PT6H\n",
      "SN19710:0  2000-01-02T12:00:00.000Z  air_temperature:2.1degC  height_above_ground:2m  PT6H\n"
     ]
    }
   ],
   "source": [
    "##Print part of the data\n",
    "for entry in raw_rdd.take(5):\n",
    "    print(entry)\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
