{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:31:33.683 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState - Setting hive.metastore.warehouse.dir ('null') to the value of spark.sql.warehouse.dir.\n",
      "21:31:33.691 [Thread-4] INFO  org.apache.spark.sql.internal.SharedState - Warehouse path is 'file:/home/ubuntu/project/spark-warehouse'.\n",
      "21:31:33.702 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "21:31:33.704 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@643751a8{/SQL,null,AVAILABLE,@Spark}\n",
      "21:31:33.705 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "21:31:33.706 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@66b520e1{/SQL/json,null,AVAILABLE,@Spark}\n",
      "21:31:33.706 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL/execution: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "21:31:33.707 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@1b73e59a{/SQL/execution,null,AVAILABLE,@Spark}\n",
      "21:31:33.707 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /SQL/execution/json: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "21:31:33.708 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@65f6744{/SQL/execution/json,null,AVAILABLE,@Spark}\n",
      "21:31:33.708 [Thread-4] INFO  org.apache.spark.ui.ServerInfo - Adding filter to /static/sql: org.apache.hadoop.yarn.server.webproxy.amfilter.AmIpFilter\n",
      "21:31:33.709 [Thread-4] INFO  org.sparkproject.jetty.server.handler.ContextHandler - Started o.s.j.s.ServletContextHandler@f91f5{/static/sql,null,AVAILABLE,@Spark}\n",
      "21:31:34.536 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.InMemoryFileIndex - It took 65 ms to list leaf files for 1 paths.\n",
      "21:31:34.746 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: parquet at NativeMethodAccessorImpl.java:0\n",
      "21:31:34.760 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 0 (parquet at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21:31:34.760 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0)\n",
      "21:31:34.761 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "21:31:34.762 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "21:31:34.784 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21:31:34.839 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0 stored as values in memory (estimated size 127.4 KiB, free 912.2 MiB)\n",
      "21:31:34.864 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_0_piece0 stored as bytes in memory (estimated size 46.8 KiB, free 912.1 MiB)\n",
      "21:31:34.866 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on namenode:37087 (size: 46.8 KiB, free: 912.3 MiB)\n",
      "21:31:34.869 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 0 from broadcast at DAGScheduler.scala:1585\n",
      "21:31:34.887 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[1] at parquet at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21:31:34.888 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 0.0 with 1 tasks resource profile 0\n",
      "21:31:34.916 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 0.0 (TID 0) (datanode2, executor 2, partition 0, PROCESS_LOCAL, 9222 bytes) \n",
      "21:31:35.205 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_0_piece0 in memory on datanode2:33447 (size: 46.8 KiB, free: 912.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:31:36.684 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 0.0 (TID 0) in 1781 ms on datanode2 (executor 2) (1/1)\n",
      "21:31:36.686 [task-result-getter-0] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 0.0, whose tasks have all completed, from pool \n",
      "21:31:36.692 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 0 (parquet at NativeMethodAccessorImpl.java:0) finished in 1.898 s\n",
      "21:31:36.696 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21:31:36.697 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 0: Stage finished\n",
      "21:31:36.699 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 0 finished: parquet at NativeMethodAccessorImpl.java:0, took 1.952953 s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:31:37.071 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on namenode:37087 in memory (size: 46.8 KiB, free: 912.3 MiB)\n",
      "21:31:37.090 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_0_piece0 on datanode2:33447 in memory (size: 46.8 KiB, free: 912.3 MiB)\n",
      "21:31:37.899 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "21:31:37.900 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "21:31:38.298 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 235.19069 ms\n",
      "21:31:38.379 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 34.534055 ms\n",
      "21:31:38.416 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1 stored as values in memory (estimated size 413.8 KiB, free 911.9 MiB)\n",
      "21:31:38.428 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_1_piece0 stored as bytes in memory (estimated size 45.0 KiB, free 911.9 MiB)\n",
      "21:31:38.428 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on namenode:37087 (size: 45.0 KiB, free: 912.3 MiB)\n",
      "21:31:38.429 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 1 from showString at NativeMethodAccessorImpl.java:0\n",
      "21:31:38.453 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4202734 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21:31:38.545 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: showString at NativeMethodAccessorImpl.java:0\n",
      "21:31:38.548 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 1 (showString at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "21:31:38.548 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 1 (showString at NativeMethodAccessorImpl.java:0)\n",
      "21:31:38.548 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "21:31:38.550 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "21:31:38.551 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21:31:38.564 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2 stored as values in memory (estimated size 18.4 KiB, free 911.8 MiB)\n",
      "21:31:38.566 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_2_piece0 stored as bytes in memory (estimated size 7.5 KiB, free 911.8 MiB)\n",
      "21:31:38.566 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on namenode:37087 (size: 7.5 KiB, free: 912.2 MiB)\n",
      "21:31:38.567 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 2 from broadcast at DAGScheduler.scala:1585\n",
      "21:31:38.568 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 12 missing tasks from ResultStage 1 (MapPartitionsRDD[6] at showString at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "21:31:38.568 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 1.0 with 12 tasks resource profile 0\n",
      "21:31:38.578 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 1.0 (TID 1) (datanode2, executor 2, partition 0, NODE_LOCAL, 9690 bytes) \n",
      "21:31:38.578 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 1.0 (TID 2) (datanode3, executor 1, partition 1, NODE_LOCAL, 9690 bytes) \n",
      "21:31:38.578 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 1.0 (TID 3) (datanode1, executor 3, partition 2, NODE_LOCAL, 9690 bytes) \n",
      "21:31:38.579 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 1.0 (TID 4) (datanode2, executor 2, partition 3, NODE_LOCAL, 9690 bytes) \n",
      "21:31:38.579 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 1.0 (TID 5) (datanode3, executor 1, partition 4, NODE_LOCAL, 9690 bytes) \n",
      "21:31:38.579 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 1.0 (TID 6) (datanode1, executor 3, partition 6, NODE_LOCAL, 9690 bytes) \n",
      "21:31:38.579 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 1.0 (TID 7) (datanode2, executor 2, partition 5, NODE_LOCAL, 9690 bytes) \n",
      "21:31:38.579 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 1.0 (TID 8) (datanode3, executor 1, partition 7, NODE_LOCAL, 9690 bytes) \n",
      "21:31:38.580 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 1.0 (TID 9) (datanode1, executor 3, partition 8, NODE_LOCAL, 9690 bytes) \n",
      "21:31:38.580 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 1.0 (TID 10) (datanode2, executor 2, partition 9, NODE_LOCAL, 9690 bytes) \n",
      "21:31:38.580 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 1.0 (TID 11) (datanode3, executor 1, partition 10, NODE_LOCAL, 9690 bytes) \n",
      "21:31:38.581 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 1.0 (TID 12) (datanode1, executor 3, partition 11, NODE_LOCAL, 9690 bytes) \n",
      "21:31:38.627 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on datanode2:33447 (size: 7.5 KiB, free: 912.3 MiB)\n",
      "21:31:38.846 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on datanode3:43161 (size: 7.5 KiB, free: 912.3 MiB)\n",
      "21:31:38.875 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_2_piece0 in memory on datanode1:38125 (size: 7.5 KiB, free: 912.3 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                        (0 + 12) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:31:39.317 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on datanode2:33447 (size: 45.0 KiB, free: 912.2 MiB)\n",
      "21:31:39.876 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 1.0 (TID 1) in 1302 ms on datanode2 (executor 2) (1/12)\n",
      "21:31:39.878 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 1.0 (TID 10) in 1298 ms on datanode2 (executor 2) (2/12)\n",
      "21:31:39.879 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 1.0 (TID 4) in 1301 ms on datanode2 (executor 2) (3/12)\n",
      "21:31:39.882 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 1.0 (TID 7) in 1302 ms on datanode2 (executor 2) (4/12)\n",
      "21:31:39.932 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on datanode3:43161 (size: 45.0 KiB, free: 912.2 MiB)\n",
      "21:31:40.001 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_1_piece0 in memory on datanode1:38125 (size: 45.0 KiB, free: 912.2 MiB)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:===================>                                      (4 + 8) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:31:41.266 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 1.0 (TID 11) in 2686 ms on datanode3 (executor 1) (5/12)\n",
      "21:31:41.267 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 1.0 (TID 8) in 2687 ms on datanode3 (executor 1) (6/12)\n",
      "21:31:41.267 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 1.0 (TID 5) in 2688 ms on datanode3 (executor 1) (7/12)\n",
      "21:31:41.268 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 1.0 (TID 2) in 2689 ms on datanode3 (executor 1) (8/12)\n",
      "21:31:41.355 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 1.0 (TID 9) in 2775 ms on datanode1 (executor 3) (9/12)\n",
      "21:31:41.361 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 1.0 (TID 6) in 2782 ms on datanode1 (executor 3) (10/12)\n",
      "21:31:41.362 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 1.0 (TID 3) in 2784 ms on datanode1 (executor 3) (11/12)\n",
      "21:31:41.362 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 1.0 (TID 12) in 2781 ms on datanode1 (executor 3) (12/12)\n",
      "21:31:41.362 [task-result-getter-0] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 1.0, whose tasks have all completed, from pool \n",
      "21:31:41.363 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 1 (showString at NativeMethodAccessorImpl.java:0) finished in 2.802 s\n",
      "21:31:41.363 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21:31:41.363 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 1: Stage finished\n",
      "21:31:41.364 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 1 finished: showString at NativeMethodAccessorImpl.java:0, took 2.818601 s\n",
      "21:31:41.408 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 19.593501 ms\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:31:42.323 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on namenode:37087 in memory (size: 7.5 KiB, free: 912.3 MiB)\n",
      "21:31:42.325 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on datanode1:38125 in memory (size: 7.5 KiB, free: 912.3 MiB)\n",
      "21:31:42.328 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on datanode3:43161 in memory (size: 7.5 KiB, free: 912.3 MiB)\n",
      "21:31:42.347 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Removed broadcast_2_piece0 on datanode2:33447 in memory (size: 7.5 KiB, free: 912.3 MiB)\n",
      "21:31:42.514 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 17.925182 ms\n",
      "+----------+-----------+--------+--------+\n",
      "|      date|temperature|feature1|feature2|\n",
      "+----------+-----------+--------+--------+\n",
      "|2000-01-01|         -6|    0.02|     1.0|\n",
      "|2000-01-02|         -1|    0.03|     1.0|\n",
      "|2000-01-03|          2|    0.05|     1.0|\n",
      "|2000-01-04|          2|    0.07|     1.0|\n",
      "|2000-01-05|         -1|    0.09|     1.0|\n",
      "|2000-01-06|          5|     0.1|    0.99|\n",
      "|2000-01-07|          4|    0.12|    0.99|\n",
      "|2000-01-08|          6|    0.14|    0.99|\n",
      "|2000-01-09|          1|    0.15|    0.99|\n",
      "|2000-01-10|          0|    0.17|    0.99|\n",
      "|2000-01-11|          3|    0.19|    0.98|\n",
      "|2000-01-12|          2|    0.21|    0.98|\n",
      "|2000-01-13|          1|    0.22|    0.98|\n",
      "|2000-01-14|         -2|    0.24|    0.97|\n",
      "|2000-01-15|         -3|    0.26|    0.97|\n",
      "|2000-01-16|          0|    0.27|    0.96|\n",
      "|2000-01-17|          8|    0.29|    0.96|\n",
      "|2000-01-18|          3|     0.3|    0.95|\n",
      "|2000-01-19|          1|    0.32|    0.95|\n",
      "|2000-01-20|          4|    0.34|    0.94|\n",
      "+----------+-----------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import sys\n",
    "from pyspark.sql import functions as F\n",
    "\n",
    "\n",
    "# Create a SparkSession with the required packages\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Data_serving_CNN\") \\\n",
    "    .config(\"spark.executor.extraPythonPackages\", \"spark_tensorflow_distributor,tensorflow\") \\\n",
    "    .config(\"spark.driver.extraPythonPackages\", \"spark_tensorflow_distributor,tensorflow\") \\\n",
    "    .config(\"spark.executorEnv.PYTHONPATH\", \":\".join(sys.path)) \\\n",
    "    .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"INFO\")\n",
    "\n",
    "df = spark.read.parquet(\"hdfs:///project/cleaned_data_parquet\")\n",
    "\n",
    "sorted_df = df.orderBy(F.col('date').asc())\n",
    "\n",
    "sorted_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21:31:42.659 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "21:31:42.659 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "21:31:42.803 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 16.960863 ms\n",
      "21:31:42.808 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3 stored as values in memory (estimated size 412.7 KiB, free 911.4 MiB)\n",
      "21:31:42.819 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_3_piece0 stored as bytes in memory (estimated size 44.8 KiB, free 911.4 MiB)\n",
      "21:31:42.820 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on namenode:37087 (size: 44.8 KiB, free: 912.2 MiB)\n",
      "21:31:42.821 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 3 from count at NativeMethodAccessorImpl.java:0\n",
      "21:31:42.824 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4202734 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21:31:42.859 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 10 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 0\n",
      "21:31:42.867 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got map stage job 2 (count at NativeMethodAccessorImpl.java:0) with 12 output partitions\n",
      "21:31:42.867 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0)\n",
      "21:31:42.868 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List()\n",
      "21:31:42.869 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "21:31:42.879 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 2 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21:31:42.903 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4 stored as values in memory (estimated size 16.9 KiB, free 911.4 MiB)\n",
      "21:31:42.905 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_4_piece0 stored as bytes in memory (estimated size 7.7 KiB, free 911.4 MiB)\n",
      "21:31:42.906 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on namenode:37087 (size: 7.7 KiB, free: 912.2 MiB)\n",
      "21:31:42.906 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 4 from broadcast at DAGScheduler.scala:1585\n",
      "21:31:42.908 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 12 missing tasks from ShuffleMapStage 2 (MapPartitionsRDD[10] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "21:31:42.908 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 2.0 with 12 tasks resource profile 0\n",
      "21:31:42.913 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 2.0 (TID 13) (datanode2, executor 2, partition 0, NODE_LOCAL, 9679 bytes) \n",
      "21:31:42.913 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 2.0 (TID 14) (datanode3, executor 1, partition 1, NODE_LOCAL, 9679 bytes) \n",
      "21:31:42.914 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 2.0 (TID 15) (datanode1, executor 3, partition 2, NODE_LOCAL, 9679 bytes) \n",
      "21:31:42.914 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 2.0 (TID 16) (datanode2, executor 2, partition 3, NODE_LOCAL, 9679 bytes) \n",
      "21:31:42.914 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 2.0 (TID 17) (datanode3, executor 1, partition 4, NODE_LOCAL, 9679 bytes) \n",
      "21:31:42.915 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 2.0 (TID 18) (datanode1, executor 3, partition 6, NODE_LOCAL, 9679 bytes) \n",
      "21:31:42.915 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 2.0 (TID 19) (datanode2, executor 2, partition 5, NODE_LOCAL, 9679 bytes) \n",
      "21:31:42.915 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 2.0 (TID 20) (datanode3, executor 1, partition 7, NODE_LOCAL, 9679 bytes) \n",
      "21:31:42.915 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 2.0 (TID 21) (datanode1, executor 3, partition 8, NODE_LOCAL, 9679 bytes) \n",
      "21:31:42.916 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 2.0 (TID 22) (datanode2, executor 2, partition 9, NODE_LOCAL, 9679 bytes) \n",
      "21:31:42.916 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 2.0 (TID 23) (datanode3, executor 1, partition 10, NODE_LOCAL, 9679 bytes) \n",
      "21:31:42.916 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 2.0 (TID 24) (datanode1, executor 3, partition 11, NODE_LOCAL, 9679 bytes) \n",
      "21:31:42.942 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on datanode2:33447 (size: 7.7 KiB, free: 912.2 MiB)\n",
      "21:31:42.961 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on datanode3:43161 (size: 7.7 KiB, free: 912.2 MiB)\n",
      "21:31:42.962 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_4_piece0 in memory on datanode1:38125 (size: 7.7 KiB, free: 912.2 MiB)\n",
      "21:31:43.048 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on datanode3:43161 (size: 44.8 KiB, free: 912.2 MiB)\n",
      "21:31:43.051 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on datanode2:33447 (size: 44.8 KiB, free: 912.2 MiB)\n",
      "21:31:43.061 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_3_piece0 in memory on datanode1:38125 (size: 44.8 KiB, free: 912.2 MiB)\n",
      "21:31:43.142 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 2.0 (TID 20) in 227 ms on datanode3 (executor 1) (1/12)\n",
      "21:31:43.149 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 2.0 (TID 23) in 233 ms on datanode3 (executor 1) (2/12)\n",
      "21:31:43.149 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 2.0 (TID 19) in 234 ms on datanode2 (executor 2) (3/12)\n",
      "21:31:43.150 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 2.0 (TID 22) in 235 ms on datanode2 (executor 2) (4/12)\n",
      "21:31:43.151 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 2.0 (TID 16) in 237 ms on datanode2 (executor 2) (5/12)\n",
      "21:31:43.153 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 2.0 (TID 24) in 237 ms on datanode1 (executor 3) (6/12)\n",
      "21:31:43.154 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 2.0 (TID 15) in 240 ms on datanode1 (executor 3) (7/12)\n",
      "21:31:43.155 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 2.0 (TID 14) in 242 ms on datanode3 (executor 1) (8/12)\n",
      "21:31:43.156 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 2.0 (TID 13) in 245 ms on datanode2 (executor 2) (9/12)\n",
      "21:31:43.156 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 2.0 (TID 17) in 242 ms on datanode3 (executor 1) (10/12)\n",
      "21:31:43.157 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 2.0 (TID 18) in 243 ms on datanode1 (executor 3) (11/12)\n",
      "21:31:43.158 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 2.0 (TID 21) in 242 ms on datanode1 (executor 3) (12/12)\n",
      "21:31:43.158 [task-result-getter-1] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 2.0, whose tasks have all completed, from pool \n",
      "21:31:43.158 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 2 (count at NativeMethodAccessorImpl.java:0) finished in 0.274 s\n",
      "21:31:43.159 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "21:31:43.159 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "21:31:43.160 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set()\n",
      "21:31:43.160 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "21:31:43.229 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 20.562328 ms\n",
      "21:31:43.251 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "21:31:43.255 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 3 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21:31:43.255 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 4 (count at NativeMethodAccessorImpl.java:0)\n",
      "21:31:43.255 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 3)\n",
      "21:31:43.255 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List()\n",
      "21:31:43.256 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 4 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21:31:43.265 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5 stored as values in memory (estimated size 12.5 KiB, free 911.4 MiB)\n",
      "21:31:43.266 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_5_piece0 stored as bytes in memory (estimated size 6.0 KiB, free 911.4 MiB)\n",
      "21:31:43.267 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on namenode:37087 (size: 6.0 KiB, free: 912.2 MiB)\n",
      "21:31:43.268 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 5 from broadcast at DAGScheduler.scala:1585\n",
      "21:31:43.268 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[13] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21:31:43.268 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 4.0 with 1 tasks resource profile 0\n",
      "21:31:43.273 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 4.0 (TID 25) (datanode1, executor 3, partition 0, NODE_LOCAL, 9010 bytes) \n",
      "21:31:43.306 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_5_piece0 in memory on datanode1:38125 (size: 6.0 KiB, free: 912.2 MiB)\n",
      "21:31:43.335 [dispatcher-event-loop-0] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint - Asked to send map output locations for shuffle 0 to 192.168.8.107:39658\n",
      "21:31:43.475 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 4.0 (TID 25) in 205 ms on datanode1 (executor 3) (1/1)\n",
      "21:31:43.476 [task-result-getter-2] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 4.0, whose tasks have all completed, from pool \n",
      "21:31:43.479 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 4 (count at NativeMethodAccessorImpl.java:0) finished in 0.216 s\n",
      "21:31:43.479 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21:31:43.479 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 4: Stage finished\n",
      "21:31:43.480 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 3 finished: count at NativeMethodAccessorImpl.java:0, took 0.228139 s\n",
      "21:31:43.610 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "21:31:43.610 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "21:31:43.640 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 12.670846 ms\n",
      "21:31:43.654 [Thread-4] INFO  org.apache.spark.sql.catalyst.expressions.codegen.CodeGenerator - Code generated in 11.487446 ms\n",
      "21:31:43.661 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6 stored as values in memory (estimated size 413.0 KiB, free 911.0 MiB)\n",
      "21:31:43.668 [Thread-4] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_6_piece0 stored as bytes in memory (estimated size 44.9 KiB, free 910.9 MiB)\n",
      "21:31:43.669 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on namenode:37087 (size: 44.9 KiB, free: 912.2 MiB)\n",
      "21:31:43.670 [Thread-4] INFO  org.apache.spark.SparkContext - Created broadcast 6 from count at NativeMethodAccessorImpl.java:0\n",
      "21:31:43.671 [Thread-4] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4202734 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21:31:43.691 [Thread-4] INFO  org.apache.spark.SparkContext - Starting job: count at NativeMethodAccessorImpl.java:0\n",
      "21:31:43.693 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 18 (count at NativeMethodAccessorImpl.java:0) as input to shuffle 1\n",
      "21:31:43.694 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 4 (count at NativeMethodAccessorImpl.java:0) with 1 output partitions\n",
      "21:31:43.694 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 6 (count at NativeMethodAccessorImpl.java:0)\n",
      "21:31:43.694 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 5)\n",
      "21:31:43.694 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 5)\n",
      "21:31:43.697 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 5 (MapPartitionsRDD[18] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21:31:43.703 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7 stored as values in memory (estimated size 22.2 KiB, free 910.9 MiB)\n",
      "21:31:43.704 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_7_piece0 stored as bytes in memory (estimated size 9.9 KiB, free 910.9 MiB)\n",
      "21:31:43.705 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on namenode:37087 (size: 9.9 KiB, free: 912.1 MiB)\n",
      "21:31:43.706 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 7 from broadcast at DAGScheduler.scala:1585\n",
      "21:31:43.706 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 12 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[18] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "21:31:43.706 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 5.0 with 12 tasks resource profile 0\n",
      "21:31:43.708 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 5.0 (TID 26) (datanode3, executor 1, partition 0, NODE_LOCAL, 9679 bytes) \n",
      "21:31:43.709 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 5.0 (TID 27) (datanode2, executor 2, partition 2, NODE_LOCAL, 9679 bytes) \n",
      "21:31:43.709 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 5.0 (TID 28) (datanode1, executor 3, partition 1, NODE_LOCAL, 9679 bytes) \n",
      "21:31:43.709 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 5.0 (TID 29) (datanode3, executor 1, partition 3, NODE_LOCAL, 9679 bytes) \n",
      "21:31:43.709 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 5.0 (TID 30) (datanode2, executor 2, partition 4, NODE_LOCAL, 9679 bytes) \n",
      "21:31:43.710 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 5.0 (TID 31) (datanode1, executor 3, partition 6, NODE_LOCAL, 9679 bytes) \n",
      "21:31:43.710 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 5.0 (TID 32) (datanode3, executor 1, partition 5, NODE_LOCAL, 9679 bytes) \n",
      "21:31:43.710 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 5.0 (TID 33) (datanode2, executor 2, partition 7, NODE_LOCAL, 9679 bytes) \n",
      "21:31:43.710 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 5.0 (TID 34) (datanode1, executor 3, partition 8, NODE_LOCAL, 9679 bytes) \n",
      "21:31:43.710 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 5.0 (TID 35) (datanode3, executor 1, partition 9, NODE_LOCAL, 9679 bytes) \n",
      "21:31:43.711 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 5.0 (TID 36) (datanode2, executor 2, partition 11, NODE_LOCAL, 9679 bytes) \n",
      "21:31:43.711 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 5.0 (TID 37) (datanode1, executor 3, partition 10, NODE_LOCAL, 9679 bytes) \n",
      "21:31:43.726 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode3:43161 (size: 9.9 KiB, free: 912.2 MiB)\n",
      "21:31:43.734 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode1:38125 (size: 9.9 KiB, free: 912.2 MiB)\n",
      "21:31:43.760 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_7_piece0 in memory on datanode2:33447 (size: 9.9 KiB, free: 912.2 MiB)\n",
      "21:31:43.777 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on datanode3:43161 (size: 44.9 KiB, free: 912.2 MiB)\n",
      "21:31:43.784 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on datanode1:38125 (size: 44.9 KiB, free: 912.1 MiB)\n",
      "21:31:43.820 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 5.0 in stage 5.0 (TID 32) in 110 ms on datanode3 (executor 1) (1/12)\n",
      "21:31:43.827 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 3.0 in stage 5.0 (TID 29) in 118 ms on datanode3 (executor 1) (2/12)\n",
      "21:31:43.831 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 5.0 (TID 26) in 123 ms on datanode3 (executor 1) (3/12)\n",
      "21:31:43.835 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 9.0 in stage 5.0 (TID 35) in 124 ms on datanode3 (executor 1) (4/12)\n",
      "21:31:43.872 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 6.0 in stage 5.0 (TID 31) in 163 ms on datanode1 (executor 3) (5/12)\n",
      "21:31:43.874 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 1.0 in stage 5.0 (TID 28) in 165 ms on datanode1 (executor 3) (6/12)\n",
      "21:31:43.874 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_6_piece0 in memory on datanode2:33447 (size: 44.9 KiB, free: 912.2 MiB)\n",
      "21:31:43.876 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 8.0 in stage 5.0 (TID 34) in 166 ms on datanode1 (executor 3) (7/12)\n",
      "21:31:43.879 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 10.0 in stage 5.0 (TID 37) in 168 ms on datanode1 (executor 3) (8/12)\n",
      "21:31:43.949 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 5.0 (TID 36) in 239 ms on datanode2 (executor 2) (9/12)\n",
      "21:31:43.956 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 4.0 in stage 5.0 (TID 30) in 247 ms on datanode2 (executor 2) (10/12)\n",
      "21:31:43.961 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 5.0 (TID 27) in 253 ms on datanode2 (executor 2) (11/12)\n",
      "21:31:43.961 [task-result-getter-2] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 7.0 in stage 5.0 (TID 33) in 251 ms on datanode2 (executor 2) (12/12)\n",
      "21:31:43.961 [task-result-getter-2] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 5.0, whose tasks have all completed, from pool \n",
      "21:31:43.962 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ShuffleMapStage 5 (count at NativeMethodAccessorImpl.java:0) finished in 0.263 s\n",
      "21:31:43.962 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - looking for newly runnable stages\n",
      "21:31:43.962 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - running: Set()\n",
      "21:31:43.962 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - waiting: Set(ResultStage 6)\n",
      "21:31:43.962 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - failed: Set()\n",
      "21:31:43.963 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ResultStage 6 (MapPartitionsRDD[22] at count at NativeMethodAccessorImpl.java:0), which has no missing parents\n",
      "21:31:43.968 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8 stored as values in memory (estimated size 22.9 KiB, free 910.9 MiB)\n",
      "21:31:43.969 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.3 KiB, free 910.9 MiB)\n",
      "21:31:43.970 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on namenode:37087 (size: 10.3 KiB, free: 912.1 MiB)\n",
      "21:31:43.970 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 8 from broadcast at DAGScheduler.scala:1585\n",
      "21:31:43.971 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[22] at count at NativeMethodAccessorImpl.java:0) (first 15 tasks are for partitions Vector(0))\n",
      "21:31:43.971 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 6.0 with 1 tasks resource profile 0\n",
      "21:31:43.972 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 6.0 (TID 38) (datanode2, executor 2, partition 0, NODE_LOCAL, 9010 bytes) \n",
      "21:31:43.990 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_8_piece0 in memory on datanode2:33447 (size: 10.3 KiB, free: 912.1 MiB)\n",
      "21:31:44.016 [dispatcher-event-loop-3] INFO  org.apache.spark.MapOutputTrackerMasterEndpoint - Asked to send map output locations for shuffle 1 to 192.168.8.30:37560\n",
      "21:31:44.161 [task-result-getter-0] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 0.0 in stage 6.0 (TID 38) in 189 ms on datanode2 (executor 2) (1/1)\n",
      "21:31:44.161 [task-result-getter-0] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Removed TaskSet 6.0, whose tasks have all completed, from pool \n",
      "Train size: 5431\n",
      "21:31:44.162 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - ResultStage 6 (count at NativeMethodAccessorImpl.java:0) finished in 0.198 s\n",
      "21:31:44.163 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 is finished. Cancelling potential speculative or zombie tasks for this job\n",
      "21:31:44.163 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Killing all running tasks in stage 6: Stage finished\n",
      "21:31:44.163 [Thread-4] INFO  org.apache.spark.scheduler.DAGScheduler - Job 4 finished: count at NativeMethodAccessorImpl.java:0, took 0.471620 s\n",
      "21:31:44.254 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "21:31:44.255 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "21:31:44.255 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Pushed Filters: \n",
      "21:31:44.255 [Thread-4] INFO  org.apache.spark.sql.execution.datasources.FileSourceStrategy - Post-Scan Filters: \n",
      "21:31:44.301 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9 stored as values in memory (estimated size 413.8 KiB, free 910.4 MiB)\n",
      "21:31:44.316 [broadcast-exchange-0] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_9_piece0 stored as bytes in memory (estimated size 45.0 KiB, free 910.4 MiB)\n",
      "21:31:44.316 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on namenode:37087 (size: 45.0 KiB, free: 912.1 MiB)\n",
      "21:31:44.317 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Created broadcast 9 from $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "21:31:44.318 [broadcast-exchange-0] INFO  org.apache.spark.sql.execution.FileSourceScanExec - Planning scan with bin packing, max size: 4202734 bytes, open cost is considered as scanning 4194304 bytes.\n",
      "21:31:44.333 [broadcast-exchange-0] INFO  org.apache.spark.SparkContext - Starting job: $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266\n",
      "21:31:44.334 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Registering RDD 27 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) as input to shuffle 2\n",
      "21:31:44.334 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Got job 5 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) with 1 output partitions\n",
      "21:31:44.334 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Final stage: ResultStage 8 ($anonfun$withThreadLocalCaptured$1 at FutureTask.java:266)\n",
      "21:31:44.334 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Parents of final stage: List(ShuffleMapStage 7)\n",
      "21:31:44.334 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Missing parents: List(ShuffleMapStage 7)\n",
      "21:31:44.336 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting ShuffleMapStage 7 (MapPartitionsRDD[27] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266), which has no missing parents\n",
      "21:31:44.338 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10 stored as values in memory (estimated size 24.5 KiB, free 910.4 MiB)\n",
      "21:31:44.340 [dag-scheduler-event-loop] INFO  org.apache.spark.storage.memory.MemoryStore - Block broadcast_10_piece0 stored as bytes in memory (estimated size 10.5 KiB, free 910.4 MiB)\n",
      "21:31:44.341 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on namenode:37087 (size: 10.5 KiB, free: 912.1 MiB)\n",
      "21:31:44.343 [dag-scheduler-event-loop] INFO  org.apache.spark.SparkContext - Created broadcast 10 from broadcast at DAGScheduler.scala:1585\n",
      "21:31:44.343 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.DAGScheduler - Submitting 12 missing tasks from ShuffleMapStage 7 (MapPartitionsRDD[27] at $anonfun$withThreadLocalCaptured$1 at FutureTask.java:266) (first 15 tasks are for partitions Vector(0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11))\n",
      "21:31:44.343 [dag-scheduler-event-loop] INFO  org.apache.spark.scheduler.cluster.YarnScheduler - Adding task set 7.0 with 12 tasks resource profile 0\n",
      "21:31:44.345 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 0.0 in stage 7.0 (TID 39) (datanode2, executor 2, partition 0, NODE_LOCAL, 9679 bytes) \n",
      "21:31:44.345 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 1.0 in stage 7.0 (TID 40) (datanode3, executor 1, partition 1, NODE_LOCAL, 9679 bytes) \n",
      "21:31:44.345 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 2.0 in stage 7.0 (TID 41) (datanode1, executor 3, partition 2, NODE_LOCAL, 9679 bytes) \n",
      "21:31:44.346 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 3.0 in stage 7.0 (TID 42) (datanode2, executor 2, partition 3, NODE_LOCAL, 9679 bytes) \n",
      "21:31:44.346 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 4.0 in stage 7.0 (TID 43) (datanode3, executor 1, partition 4, NODE_LOCAL, 9679 bytes) \n",
      "21:31:44.346 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 6.0 in stage 7.0 (TID 44) (datanode1, executor 3, partition 6, NODE_LOCAL, 9679 bytes) \n",
      "21:31:44.347 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 5.0 in stage 7.0 (TID 45) (datanode2, executor 2, partition 5, NODE_LOCAL, 9679 bytes) \n",
      "21:31:44.347 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 7.0 in stage 7.0 (TID 46) (datanode3, executor 1, partition 7, NODE_LOCAL, 9679 bytes) \n",
      "21:31:44.347 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 8.0 in stage 7.0 (TID 47) (datanode1, executor 3, partition 8, NODE_LOCAL, 9679 bytes) \n",
      "21:31:44.348 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 9.0 in stage 7.0 (TID 48) (datanode2, executor 2, partition 9, NODE_LOCAL, 9679 bytes) \n",
      "21:31:44.348 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 10.0 in stage 7.0 (TID 49) (datanode3, executor 1, partition 10, NODE_LOCAL, 9679 bytes) \n",
      "21:31:44.348 [dispatcher-CoarseGrainedScheduler] INFO  org.apache.spark.scheduler.TaskSetManager - Starting task 11.0 in stage 7.0 (TID 50) (datanode1, executor 3, partition 11, NODE_LOCAL, 9679 bytes) \n",
      "21:31:44.368 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on datanode1:38125 (size: 10.5 KiB, free: 912.1 MiB)\n",
      "21:31:44.372 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on datanode3:43161 (size: 10.5 KiB, free: 912.1 MiB)\n",
      "21:31:44.377 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_10_piece0 in memory on datanode2:33447 (size: 10.5 KiB, free: 912.1 MiB)\n",
      "21:31:44.393 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on datanode3:43161 (size: 45.0 KiB, free: 912.1 MiB)\n",
      "21:31:44.398 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on datanode1:38125 (size: 45.0 KiB, free: 912.1 MiB)\n",
      "21:31:44.401 [dispatcher-BlockManagerMaster] INFO  org.apache.spark.storage.BlockManagerInfo - Added broadcast_9_piece0 in memory on datanode2:33447 (size: 45.0 KiB, free: 912.1 MiB)\n",
      "21:31:44.546 [task-result-getter-3] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 2.0 in stage 7.0 (TID 41) in 201 ms on datanode1 (executor 3) (1/12)\n",
      "21:31:44.552 [task-result-getter-1] INFO  org.apache.spark.scheduler.TaskSetManager - Finished task 11.0 in stage 7.0 (TID 50) in 204 ms on datanode1 (executor 3) (2/12)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "\n",
    "total_count = sorted_df.count()\n",
    "\n",
    "train_size = int(total_count * 0.6)\n",
    "val_size = int(total_count * 0.2)\n",
    "test_size = int(total_count * 0.2)\n",
    "\n",
    "train_df = sorted_df.limit(train_size).orderBy(F.col('date').asc())\n",
    "val_df = sorted_df.limit(train_size + val_size).subtract(train_df).orderBy(F.col('date').asc())\n",
    "test_df = sorted_df.subtract(train_df).subtract(val_df).orderBy(F.col('date').asc())\n",
    "\n",
    "# Show sizes of the splits\n",
    "print(f\"Train size: {train_df.count()}\")\n",
    "print(f\"Validation size: {val_df.count()}\")\n",
    "print(f\"Test size: {test_df.count()}\")  \n",
    "\n",
    "\n",
    "train_df.show()\n",
    "#Print one row of the dataframe\n",
    "print(train_df.first())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import DataFrame\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "def create_sequences(df, input_length=30, output_length=1):\n",
    "    # Prepare the LSTM input and output in a way that TensorFlow can understand\n",
    "    sequences = []\n",
    "    targets = []\n",
    "    \n",
    "    data = df.collect()\n",
    "\n",
    "    \n",
    "    for i in range(len(data) - input_length - output_length + 1):\n",
    "        # Extract features for input\n",
    "        sequence = [[data[j][1], round(data[j][2], 2), round(data[j][3], 2)] for j in range(i, i + input_length)]  # Using temperature, feature1, feature2\n",
    "        sequences.append(sequence)\n",
    "        \n",
    "        # Extract target for output (next 7 days temperature)\n",
    "        target = [data[i + input_length + j][1] for j in range(output_length)]  # Next 7 days\n",
    "        targets.append(target)\n",
    "    \n",
    "    return sequences, targets\n",
    "\n",
    "def create_date_sequence(df, input_length=30, output_length=1):\n",
    "    dates = []\n",
    "    \n",
    "    data = df.collect()\n",
    "\n",
    "    \n",
    "    for i in range(len(data) - input_length - output_length + 1):\n",
    "        date = [datetime.strptime(data[i + input_length + j][0], '%Y-%m-%d') for j in range(output_length)]  # Next 7 days\n",
    "        for j in range(len(date)):\n",
    "            dates.append(date[j])\n",
    "    \n",
    "    return dates\n",
    "\n",
    "\n",
    "x_train, y_train = create_sequences(train_df)\n",
    "\n",
    "x_val, y_val = create_sequences(val_df)\n",
    "\n",
    "x_test, y_test = create_sequences(test_df)\n",
    "\n",
    "y_test_dates = create_date_sequence(test_df)\n",
    "\n",
    "print(x_train[-1])\n",
    "print(y_train[-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from spark_tensorflow_distributor import MirroredStrategyRunner\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "def train():\n",
    "    BATCH_SIZE = 64\n",
    "    EPOCHS = 200\n",
    "\n",
    "    def make_datasets():\n",
    "        global x_train, y_train, x_val, y_val\n",
    "        \n",
    "        train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(BATCH_SIZE)\n",
    "        val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val)).batch(BATCH_SIZE)\n",
    "        \n",
    "        options = tf.data.Options()\n",
    "        options.experimental_distribute.auto_shard_policy = tf.data.experimental.AutoShardPolicy.DATA\n",
    "        train_dataset = train_dataset.with_options(options)\n",
    "        val_dataset = val_dataset.with_options(options)\n",
    "        \n",
    "        return train_dataset, val_dataset\n",
    "\n",
    "    def make_test_dataset():\n",
    "        global x_test, y_test\n",
    "        test_dataset = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(BATCH_SIZE)\n",
    "        return test_dataset\n",
    "\n",
    "    # https://thejaskiran99.medium.com/unlocking-the-potential-of-convolutional-neural-networks-cnns-in-time-series-forecasting-b2fac329e184\n",
    "    def build_and_compile_cnn_model():\n",
    "        model = tf.keras.Sequential([\n",
    "            tf.keras.layers.Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(30, 3)),\n",
    "            tf.keras.layers.MaxPooling1D(pool_size=2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(50, activation='relu'),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Dense(1)\n",
    "        ])\n",
    "        model.compile(\n",
    "            loss=\"mean_squared_error\",\n",
    "            optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "            metrics=[\"mae\", \"mse\", \"accuracy\"]\n",
    "        )\n",
    "        return model\n",
    "\n",
    "    # Load datasets\n",
    "    train_datasets, val_datasets = make_datasets()\n",
    "    test_datasets = make_test_dataset()\n",
    "\n",
    "    # Build model\n",
    "    model = build_and_compile_cnn_model()\n",
    "    \n",
    "    # Early stopping callback to prevent overfitting\n",
    "    early_stopping_cb = tf.keras.callbacks.EarlyStopping(\n",
    "        monitor=\"val_loss\",\n",
    "        patience=10,\n",
    "        restore_best_weights=True\n",
    "    )\n",
    "       \n",
    "    # Train model\n",
    "    model.fit(\n",
    "        x=train_datasets, \n",
    "        epochs=EPOCHS, \n",
    "        verbose=1,\n",
    "        validation_data=val_datasets,\n",
    "        callbacks=[early_stopping_cb]\n",
    "    )\n",
    "\n",
    "    # Evaluate and calculate metrics\n",
    "    results= model.evaluate(test_datasets, return_dict=True)\n",
    "    y_pred = model.predict(test_datasets)\n",
    "    \n",
    "    return {\n",
    "        \"MAE\": float(results[\"mae\"]),\n",
    "        \"MSE\": float(results[\"mse\"]),\n",
    "        \"Accuracy\": float(results[\"accuracy\"]),\n",
    "        \"RMSE\": np.sqrt(results[\"mse\"]),\n",
    "        \"y_true\": y_test,\n",
    "        \"y_pred\": y_pred\n",
    "    }\n",
    "\n",
    "# Run distributed training\n",
    "runner = MirroredStrategyRunner(num_slots=3, use_gpu=False)\n",
    "metrics_results = runner.run(train)\n",
    "\n",
    "# Print results\n",
    "print(\"\\nMetrics Results:\")\n",
    "print(\"MAE: \", metrics_results[\"MAE\"])\n",
    "print(\"MSE: \", metrics_results[\"MSE\"])\n",
    "print(\"Accuracy: \", metrics_results[\"Accuracy\"])\n",
    "print(\"RMSE: \", metrics_results[\"RMSE\"])\n",
    " \n",
    "y_true = np.array(metrics_results[\"y_true\"])\n",
    "y_pred = np.array(metrics_results[\"y_pred\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "dates = test_df.select(\"date\").collect()\n",
    "true_values = test_df.select(\"temperature\").collect()\n",
    "true_values = true_values[30:]\n",
    "\n",
    "# Got from openAi by asking to plot the dates in x-axis and giving the date format to the model\n",
    "dates = dates[30:]\n",
    "dates = [datetime.strptime(date[0], '%Y-%m-%d') for date in dates]\n",
    "\n",
    "# Create arrays for predictions\n",
    "all_pred_dates = []\n",
    "\n",
    "predictions = {}\n",
    "\n",
    "y_pred_mean = []\n",
    "for i in range(len(y_pred)):\n",
    "    for j in range(len(y_pred[i])):\n",
    "        if dates[i+j] not in predictions:\n",
    "            predictions[dates[i+j]] = []\n",
    "        predictions[dates[i+j]].append(y_pred[i][j])\n",
    "\n",
    "for date, preds in predictions.items():\n",
    "    y_pred_mean.append(np.mean(preds))\n",
    "   \n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(dates, true_values, label=\"Actual\", color='blue')\n",
    "plt.plot(dates, y_pred_mean, color='red', label=\"Predictions\")\n",
    "plt.xticks(ticks=dates[::30], labels=[date.strftime('%Y-%m-%d') for date in dates[::30]], rotation=45)\n",
    "plt.xlabel(\"Date\")\n",
    "plt.ylabel(\"Temperature\")\n",
    "plt.title(\"Temperature Over Time: Actual vs Predicted\")\n",
    "plt.legend()\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
