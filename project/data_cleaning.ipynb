{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2. Data Cleaning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SN19710:0  2000-01-01T06:00:00.000Z  air_temperature:-6.3degC  height_above_ground:2m  PT6H\n",
      "SN19710:0  2000-01-01T12:00:00.000Z  air_temperature:-4degC  height_above_ground:2m  PT6H\n",
      "SN19710:0  2000-01-01T18:00:00.000Z  air_temperature:-7degC  height_above_ground:2m  PT6H\n",
      "SN19710:0  2000-01-02T06:00:00.000Z  air_temperature:-4.4degC  height_above_ground:2m  PT6H\n",
      "SN19710:0  2000-01-02T12:00:00.000Z  air_temperature:2.1degC  height_above_ground:2m  PT6H\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"data_cleaning\").getOrCreate()\n",
    "\n",
    "# Load the saved data from the directory\n",
    "loaded_rdd = spark.sparkContext.textFile(\"hdfs:///project/raw_temperature_data\")\n",
    "\n",
    "# Print the first 5 entries to verify it was saved and loaded correctly\n",
    "for i in loaded_rdd.take(5):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Process the raw data line by line and convert it into a structured format containing date as the ky and temperature and day of the year as values. Since the data is hourly we convert the datetime to date"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2000-01-01', (-6.3, 0.02, 1.0))\n",
      "('2000-01-01', (-4.0, 0.02, 1.0))\n",
      "('2000-01-01', (-7.0, 0.02, 1.0))\n",
      "('2000-01-02', (-4.4, 0.03, 1.0))\n",
      "('2000-01-02', (2.1, 0.03, 1.0))\n",
      "('2000-01-02', (-0.8, 0.03, 1.0))\n",
      "('2000-01-03', (0.4, 0.05, 1.0))\n",
      "('2000-01-03', (3.2, 0.05, 1.0))\n",
      "('2000-01-03', (2.5, 0.05, 1.0))\n",
      "('2000-01-04', (2.2, 0.07, 1.0))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "import math\n",
    "\n",
    "# Define a function to process each line of the log file and extract date as a key and temperature and day of the year as values\n",
    "def process_line(line):\n",
    "    fields = line.split(\"  \")\n",
    "    if len(fields) != 5:\n",
    "        return None  # Invalid format, skip this line\n",
    "\n",
    "    source_id, ref_time, temperature, height, time_res = [f.strip() for f in fields]\n",
    "\n",
    "    # Extract date and temperature\n",
    "    date = ref_time.split(\"T\")[0]  # Assumes ISO format like '2024-10-01T00:00:00.000Z'\n",
    "    temp_value = float(temperature.split(\":\")[1].rstrip(\"degC\"))\n",
    "\n",
    "    # Convert date string to datetime object\n",
    "    date_obj = datetime.strptime(date, \"%Y-%m-%d\")\n",
    "\n",
    "    # Calculate day of the year\n",
    "    day_of_year = date_obj.timetuple().tm_yday\n",
    "\n",
    "    # Apply sine/cosine transformation to day of the year to capture seasonality\n",
    "    day_of_year_sin = round(math.sin(2 * math.pi * day_of_year / 365), 2)\n",
    "    day_of_year_cos = round(math.cos(2 * math.pi * day_of_year / 365), 2)\n",
    "    \n",
    "    return (date, (temp_value, day_of_year_sin, day_of_year_cos))\n",
    "\n",
    "    \n",
    "\n",
    "    # Process the loaded RDD to extract the required information\n",
    "    \n",
    "\n",
    "processed_rdd = loaded_rdd.map(process_line).filter(lambda x: x is not None)\n",
    "\n",
    "# Print the first 10 entries to verify the processing\n",
    "for i in processed_rdd.take(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Since the data is in hourly format, and we above converted it to daily format, we need to aggregate the data to get the average temperature for each day.\n",
    "\n",
    "### Profile the performance of the MapReduce implementation (e.g., Spark job execution time, memory usage)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MapReduce job execution time: 0.030730724334716797 seconds\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 2:======================================>                   (8 + 4) / 12]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('2000-01-20', (4, 0.34, 0.94))\n",
      "('2000-02-17', (-4, 0.74, 0.68))\n",
      "('2000-02-18', (-5, 0.75, 0.66))\n",
      "('2000-02-24', (-1, 0.81, 0.58))\n",
      "('2000-02-27', (3, 0.84, 0.54))\n",
      "('2000-03-01', (2, 0.87, 0.5))\n",
      "('2000-03-11', (0, 0.94, 0.34))\n",
      "('2000-03-12', (-2, 0.95, 0.33))\n",
      "('2000-03-15', (1, 0.96, 0.28))\n",
      "('2000-03-18', (4, 0.97, 0.23))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "import time\n",
    "import statistics\n",
    "\n",
    "# Measure the execution time of the MapReduce job\n",
    "start_time = time.time()\n",
    "\n",
    "import statistics\n",
    "\n",
    "# Perform the MapReduce operation with rounding to whole numbers\n",
    "daily_avg_rdd = processed_rdd.groupByKey().mapValues(lambda temps: list(temps)) \\\n",
    "    .mapValues(lambda temps: (round(statistics.mean([t[0] for t in temps])), temps[0][1], temps[0][2]))\n",
    "\n",
    "\n",
    "# # Trigger an action to force execution\n",
    "# daily_avg_rdd.count()\n",
    "\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"MapReduce job execution time: {execution_time} seconds\")\n",
    "\n",
    "# To monitor memory usage, you can use the Spark UI. \n",
    "# The Spark UI is usually available at http://<driver-node>:4040\n",
    "# You can access it by navigating to the URL in your web browser.\n",
    "\n",
    "# Print the first 10 entries to verify the processing\n",
    "for i in daily_avg_rdd.take(10):\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### If there is gaps in the data, fill the missing values with the average of the previous and next value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# min_max_dates = processed_rdd.map(lambda x: x[1][1]).min(), processed_rdd.map(lambda x: x[1][1]).max()\n",
    "# start_day, end_day = min_max_dates\n",
    "# complete_days = spark.sparkContext.parallelize(range(start_day, end_day + 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transformthe data to a data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----------+--------+--------+\n",
      "|      date|temperature|feature1|feature2|\n",
      "+----------+-----------+--------+--------+\n",
      "|2013-03-09|         -6|    0.92|    0.39|\n",
      "|2013-03-13|         -7|    0.95|    0.33|\n",
      "|2013-03-14|         -8|    0.95|    0.31|\n",
      "|2013-04-14|          2|    0.98|   -0.22|\n",
      "|2013-04-15|          5|    0.97|   -0.23|\n",
      "|2013-04-26|          6|    0.91|   -0.41|\n",
      "|2013-05-24|         12|    0.62|   -0.79|\n",
      "|2013-06-13|         13|    0.31|   -0.95|\n",
      "|2013-06-17|         15|    0.25|   -0.97|\n",
      "|2013-07-21|         21|   -0.33|   -0.94|\n",
      "|2013-09-03|         15|   -0.89|   -0.46|\n",
      "|2013-09-05|         15|    -0.9|   -0.43|\n",
      "|2013-09-11|         14|   -0.94|   -0.33|\n",
      "|2013-10-18|          4|   -0.96|    0.29|\n",
      "|2013-10-30|          5|   -0.88|    0.48|\n",
      "|2013-11-19|          2|   -0.66|    0.75|\n",
      "|2013-12-03|          4|   -0.46|    0.89|\n",
      "|2013-12-26|          3|   -0.09|     1.0|\n",
      "|2014-01-14|         -8|    0.24|    0.97|\n",
      "|2014-02-02|          1|    0.54|    0.84|\n",
      "+----------+-----------+--------+--------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import StructType, StructField, StringType, IntegerType, FloatType\n",
    "\n",
    "\n",
    "formatted_rdd = daily_avg_rdd.map(lambda x: (x[0], x[1][0], x[1][1], x[1][2]))\n",
    "\n",
    "# Define the schema for your data\n",
    "schema = StructType([\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"temperature\", IntegerType(), True),\n",
    "    StructField(\"feature1\", FloatType(), True),\n",
    "    StructField(\"feature2\", FloatType(), True)\n",
    "])\n",
    "\n",
    "## Convert the RDD to a DataFrame\n",
    "df = spark.createDataFrame(formatted_rdd, schema)\n",
    "\n",
    "# Show the first few rows to verify\n",
    "df.show()\n",
    "\n",
    "df.write.mode(\"overwrite\").parquet(\"hdfs:///project/cleaned_data_parquet\")\n",
    "\n",
    "spark.stop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
